{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from mlxtend.preprocessing import minmax_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/virat/Desktop/7150/train_data.csv\", index_col=0)\n",
    "test = pd.read_csv(\"C:/Users/virat/Desktop/7150/test_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d976fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c174c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with their dataset\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describes the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db797ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the .csv file is opened in excel11-20 is converted to Nov-20\n",
    "train = train.replace('Nov-20', '11-20')\n",
    "test = test.replace('Nov-20', '11-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values using statistics module\n",
    "train['Bed Grade'].fillna(statistics.mode(train['Bed Grade']),inplace=True)\n",
    "train['City_Code_Patient'].fillna(statistics.mode(train['City_Code_Patient']),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns are of no use so we are removing them\n",
    "train.drop(['case_id', 'patientid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't predict 11 classes so we are merging them into 3 classes to predict\n",
    "train['Stay'].replace('More than 100 Days', '>100', inplace=True)\n",
    "train['Stay']= train['Stay'].replace(\n",
    "    {'0-10':0, '11-20':0, '21-30':1, '31-40':1, '41-50':1, '51-60':2,'61-70':2,'71-80':2,'81-90':2,'91-100':2,'>100':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing columns into categorical and numerical columns\n",
    "cat_cols=[]\n",
    "num_cols=[]\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtypes=='object':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of count of values into each variables\n",
    "i=1\n",
    "plt.figure(figsize=(15,20))\n",
    "for col in cat_cols:\n",
    "    plt.subplot(5,2,i)\n",
    "    sns.countplot(train[col])\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of density of values into each variables\n",
    "i=1\n",
    "plt.figure(figsize=(15,20))\n",
    "for col in num_cols:\n",
    "    plt.subplot(4,2,i)\n",
    "    sns.distplot(train[col])\n",
    "    i=i+1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2148a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing some columns from numerical type to categorical as they fit better\n",
    "cat_cols.append('Bed Grade')\n",
    "cat_cols.append('City_Code_Hospital')\n",
    "cat_cols.append('City_Code_Patient')\n",
    "\n",
    "num_cols.remove('Bed Grade')\n",
    "num_cols.remove('City_Code_Hospital')\n",
    "num_cols.remove('City_Code_Patient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical columns\n",
    "le= LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    train[col]= le.fit_transform(train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing numerical columns\n",
    "ss= StandardScaler()\n",
    "train[num_cols]= ss.fit_transform(train[num_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff281401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of dataset variables\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(train.corr(), annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting skewness of these 3 columns\n",
    "num_data = train[['Available Extra Rooms in Hospital', 'Bed Grade', 'Admission_Deposit']]\n",
    "fig, ax =plt.subplots(2,2, figsize=(14,10))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "for ax, n in zip(ax.flatten(), num_data.columns.tolist()):\n",
    "    sns.distplot(ax=ax, a=num_data[n].dropna(), label=\"Skewness : %.2f\"%(num_data[n].skew()))\n",
    "    ax.set_title(n, fontsize = 14)\n",
    "    ax.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing predicting columns from training file\n",
    "y= train['Stay']\n",
    "X= train.drop('Stay', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b6fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-plotting to see outliers in dataset\n",
    "sns.boxplot(x = 'Stay', y = 'Age', data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scaling of dataset\n",
    "scaled_data = minmax_scaling(train['Age'], columns=[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(train['Age'], ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting normalization of dataset\n",
    "normalized_data = stats.boxcox(train['Stay'])\n",
    "\n",
    "fig, ax=plt.subplots(1, 2, figsize=(15, 3))\n",
    "sns.histplot(train['Stay'], ax=ax[0], kde=True, legend=False)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.histplot(normalized_data[0], ax=ax[1], kde=True, legend=False)\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing k-values to see the difference\n",
    "kf = KFold(n_splits = 2)\n",
    "# kf = KFold(n_splits = 3)\n",
    "# kf = KFold(n_splits = 5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "print(precision_score(y_test,y_pred, average='macro'))\n",
    "print(recall_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "print(precision_score(y_test,y_pred, average='macro'))\n",
    "print(recall_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac91b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running K-Nearest Neighbours\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "print(precision_score(y_test,y_pred, average='macro'))\n",
    "print(recall_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Gradient Boosting, this is the slowest\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train,y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "print(precision_score(y_test,y_pred, average='macro'))\n",
    "print(recall_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a830752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Extreme Gradient Boosting\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion)\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "\n",
    "print(precision_score(y_test,y_pred, average='macro'))\n",
    "print(recall_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(f1_score(y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of dataset normally\n",
    "X_train, X_test, y_train,y_test= train_test_split(X,y,test_size= 0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff251e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running all the five algorithms\n",
    "modells = [LogisticRegression(max_iter=1000), RandomForestClassifier(), KNeighborsClassifier(), \n",
    "           GradientBoostingClassifier(), XGBClassifier()]\n",
    "\n",
    "name = ['LogisticRegression', 'RandomForsetClassifier', 'KNeighborsClassifier', \n",
    "        'GradientBoostingClassifier', 'XGBClassifier']\n",
    "\n",
    "models= dict(zip(name,modells))\n",
    "accuracy_scores=[]\n",
    "for key,value in models.items():\n",
    "    value.fit(X_train,y_train)\n",
    "    y_pred= value.predict(X_test)\n",
    "    accuracy= accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print(key)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing all the models and plotting\n",
    "sns.barplot(x= ['LR','RF','KNN','GBC','XGB'],y=accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61924026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning Logistic Regression\n",
    "model1 = LogisticRegression(max_iter=1000)\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "params1 = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "grid_search1 = RandomizedSearchCV(estimator=model1,  cv=3, param_distributions=params1, n_iter=5)\n",
    "grid_result1 = grid_search1.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning Random Forest\n",
    "model2 = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "\n",
    "params2 = dict(n_estimators=n_estimators,max_features=max_features)\n",
    "grid_search2 = RandomizedSearchCV(estimator=model2,  cv=3, param_distributions=params2, n_iter=5)\n",
    "grid_result2 = grid_search2.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f008c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning K-Nearest Neighbours\n",
    "model3 = KNeighborsClassifier()\n",
    "n_neighbors = range(1, 21, 2)\n",
    "weights = ['uniform', 'distance']\n",
    "metric = ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "params3 = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
    "grid_search3 = RandomizedSearchCV(estimator=model3,  cv=3, param_distributions=params3, n_iter=5)\n",
    "grid_result3 = grid_search3.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result3.best_score_, grid_result3.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning Gradient Boosting, this is sooooo slow\n",
    "model4 = GradientBoostingClassifier()\n",
    "n_estimators2 = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "\n",
    "params4 = dict(learning_rate=learning_rate, n_estimators=n_estimators2, subsample=subsample, max_depth=max_depth)\n",
    "grid_search4 = RandomizedSearchCV(estimator=model4,  cv=3, param_distributions=params4, n_iter=5)\n",
    "grid_result4 = grid_search4.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result4.best_score_, grid_result4.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf55ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning Extreme Gradient Boosting\n",
    "model5 = XGBClassifier(n_estimators=1000)\n",
    "objective = ['binary:logistic']\n",
    "max_depth2 = [3,4,5,6]\n",
    "min_child_weight = [1,5,10,12]\n",
    "subsample2 = [0.6,0.8,1.0]\n",
    "colsample_bytree = [0.6,0.8,1.0]\n",
    "gamma = [0.5,1,1.5,2]\n",
    "\n",
    "params5 = dict(objective=objective, max_depth=max_depth2, min_child_weight=min_child_weight, subsample=subsample2,\n",
    "            colsample_bytree=colsample_bytree, gamma=gamma)\n",
    "grid_search5 = RandomizedSearchCV(estimator=model5,  cv=3, param_distributions=params5, n_iter=5)\n",
    "grid_result5 = grid_search5.fit(X, y)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result5.best_score_, grid_result5.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting to see performance of all models (can't run it, crashes my system)\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
